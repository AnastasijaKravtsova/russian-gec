{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kravtsova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kravtsova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/kravtsova/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/kravtsova/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-03 13:54:47.377 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /cephfs/home/kravtsova/.deeppavlov/models/morpho_ru_syntagrus/tag.dict]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:236: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:314: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:75: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:571: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/models/bert/bert_sequence_tagger.py:249: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-03 13:55:09.8 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /cephfs/home/kravtsova/.deeppavlov/models/morpho_ru_syntagrus/model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /cephfs/home/kravtsova/.deeppavlov/models/morpho_ru_syntagrus/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-03 13:55:13.183 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 52: No load path is set for UDPymorphyLemmatizer!\n",
      "2020-06-03 13:55:13.745 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /cephfs/home/kravtsova/.deeppavlov/models/syntax_ru_syntagrus/deps.dict]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/core/layers/tf_layers.py:157: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/core/layers/tf_layers.py:177: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /cephfs/home/kravtsova/venv/lib/python3.6/site-packages/deeppavlov/core/layers/tf_layers.py:181: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-03 13:55:35.212 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /cephfs/home/kravtsova/.deeppavlov/models/syntax_ru_syntagrus/model_joint]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /cephfs/home/kravtsova/.deeppavlov/models/syntax_ru_syntagrus/model_joint\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model, configs\n",
    "model = build_model(configs.syntax.ru_syntagrus_joint_parsing, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tВосемь\tвосемь\tNUM\t_\tCase=Acc\t2\tnummod:gov\t_\t_\n",
      "2\tдней\tдень\tNOUN\t_\tAnimacy=Inan|Case=Gen|Gender=Masc|Number=Plur\t6\tobl\t_\t_\n",
      "3\tспустя\tспустя\tADP\t_\t_\t2\tcase\t_\t_\n",
      "4\tи\tи\tPART\t_\t_\t5\tadvmod\t_\t_\n",
      "5\tнационалисты\tнационалист\tNOUN\t_\tAnimacy=Anim|Case=Nom|Gender=Masc|Number=Plur\t6\tnsubj\t_\t_\n",
      "6\tнападали\tнападать\tVERB\t_\tAspect=Imp|Mood=Ind|Number=Plur|Tense=Past|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
      "7\tна\tна\tADP\t_\t_\t8\tcase\t_\t_\n",
      "8\tКаталонию\tкаталония\tPROPN\t_\tAnimacy=Inan|Case=Acc|Gender=Fem|Number=Sing\t6\tobl\t_\t_\n",
      "9\t.\t.\tPUNCT\t_\t_\t6\tpunct\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Восемь дней спустя и националисты нападали на Каталонию .\"]\n",
    "for parse in model(sentences):\n",
    "    print(parse, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RULEC-GEC_test_original_ya_corrected_parsed.txt', 'r', encoding='utf8') as f:\n",
    "    test_ya_parsed = f.read()\n",
    "test_parsed = test_ya_parsed.split('\\n\\n')\n",
    "del test_parsed[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skills_2/RULEC-GEC_test_ya_speller_punct_comma-masked_prep-masked_parsed.txt', 'r', encoding='utf8') as f:\n",
    "    test_ya_parsed = f.read()\n",
    "test_parsed = test_ya_parsed.split('\\n\\n')\n",
    "del test_parsed[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RULEC-GEC_test_original_ya_corrected.txt', 'r', encoding='utf8') as f:\n",
    "    test = f.readlines()\n",
    "test = [x.strip() for x in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check noun wordforms (in case of incorrect morphotags) -- just test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 4709/4993 [00:05<00:00, 938.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lazzzzzzhhhhaaa\n",
      "['9', 'шахматы', 'шахматы', 'NOUN', '_', 'Animacy=Inan|Case=Dat|Gender=Fem|Number=Sing', '7', 'nmod', '_', '_']\n",
      "[Parse(word='шахматы', tag=OpencorporaTag('NOUN,inan,GNdr,Pltm plur,nomn'), normal_form='шахматы', score=0.75, methods_stack=((<DictionaryAnalyzer>, 'шахматы', 209, 0),))]\n",
      "['datv', 'sing']\n",
      "[]\n",
      "1\tУ\tу\tADP\t_\t_\t2\tcase\t_\t_\n",
      "2\tнас\tмы\tPRON\t_\tCase=Gen|Number=Plur|Person=1\t6\tobl\t_\t_\n",
      "3\tуже\tуже\tADV\t_\tDegree=Pos\t4\tadvmod\t_\t_\n",
      "4\tдва\tдва\tNUM\t_\tAnimacy=Inan|Case=Acc|Gender=Masc\t5\tnummod:gov\t_\t_\n",
      "5\tгода\tгод\tNOUN\t_\tAnimacy=Inan|Case=Gen|Gender=Masc|Number=Sing\t6\tobl\t_\t_\n",
      "6\tидёт\tидти\tVERB\t_\tAspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
      "7\tсоревнование\tсоревнование\tNOUN\t_\tAnimacy=Inan|Case=Nom|Gender=Neut|Number=Sing\t6\tnsubj\t_\t_\n",
      "8\tпо\tпо\tADP\t_\t_\t9\tcase\t_\t_\n",
      "9\tшахматы\tшахматы\tNOUN\t_\tAnimacy=Inan|Case=Dat|Gender=Fem|Number=Sing\t7\tnmod\t_\t_\n",
      "10\t.\t.\tPUNCT\t_\t_\t6\tpunct\t_\t_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4993/4993 [00:05<00:00, 899.56it/s] \n"
     ]
    }
   ],
   "source": [
    "for sent in tqdm(test_parsed):\n",
    "    spl = sent.split('\\n')\n",
    "    synt_info = [x.split('\\t') for x in spl]\n",
    "    i = 0\n",
    "    for i in range(len(synt_info)):\n",
    "        tok = synt_info[i]\n",
    "        if (tok[3] == 'NOUN') and tok[1][0].islower():\n",
    "            first_letter = tok[1][0]\n",
    "            noun_features = tok[5].split('|')\n",
    "            try:\n",
    "                lexem = morph.parse(tok[2])\n",
    "                lex = [x for x in lexem if 'NOUN' in x.tag and 'nomn' in x.tag]\n",
    "                if lex:\n",
    "                    if 'Sgtm' in str(lex[0].tag):\n",
    "    #                     i += 1\n",
    "                        continue\n",
    "                    fs = [features_dict[x.split('=')[1]] for x in noun_features if not x.startswith('Animacy') and not x.startswith('Gender')]\n",
    "                    gender = [x for x in ['masc', 'femn', 'neut'] if x in str(lex[0].tag)]\n",
    "                    fs.extend(gender)\n",
    "                    if 'plur' in fs:\n",
    "                        if 'masc' in fs:\n",
    "                            fs.remove('masc')\n",
    "                        elif 'femn' in fs:\n",
    "                            fs.remove('femn')\n",
    "                        elif 'neut' in fs:\n",
    "                            fs.remove('neut')\n",
    "                    test = first_letter + lex[0].inflect(set(fs)).word[1:]\n",
    "                    synt_info[i][1] = test\n",
    "    #                 print(test)\n",
    "            except AttributeError:\n",
    "                print('lazzzzzzhhhhaaa')\n",
    "                print(tok)\n",
    "                print(lex)\n",
    "                print(fs)\n",
    "                print(gender)\n",
    "                print(sent)\n",
    "#         i += 1\n",
    "    new_sent = ' '.join(x[1] for x in synt_info)\n",
    "#     print(new_sent)\n",
    "    with open('skills_2/RULEC-GEC_test_ya_speller_punct_comma-masked_prep-masked_nouns-tags-check_test.txt', 'a', encoding='utf8') as f:\n",
    "        print(new_sent, file=f, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun agreement with prep on syntactic tree using prep case -- skills_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {'Nom':'nomn', 'Gen':'gent', 'Dat':'datv', 'Acc':'accs', 'Ins':'ablt', 'Loc':'loct', 'Voc':'voct',\n",
    "       'Par':'gen2', 'Sing':'sing', 'Plur':'plur', 'Masc':'masc', 'Fem':'femn', 'Neut':'neut', 'Sup':'Supr',\n",
    "                 'Anim':'anim', 'Inan':'inan',\n",
    "                'Short':'ADJS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='досада', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='досада', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'досада', 55, 0),))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex = morph.parse('досада')[0]\n",
    "lex\n",
    "# 'ADJF' in lex.tag\n",
    "# lex.inflect({'femn', 'sing', 'ablt'}).word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "preps_data = pd.read_csv('rus_preps.csv')\n",
    "preps = preps_data['word'].tolist()\n",
    "cases = preps_data['case'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_match = {'acc':'Case=Acc', 'dat':'Case=Dat', 'gen':'Case=Gen', 'inst':'Case=Ins', 'prep':'Case=Loc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cases = [cases_match.get(item, item) for item in cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps_cases = dict(zip(preps, new_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps_cases['согласно'] = 'Case=Dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preps_cases['спустя'] = 'Case=Acc,Case=Gen'\n",
    "preps_cases['спустя'] = 'Case=Acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps_cases['в'] = 'Case=Acc,Case=Loc' # не нужен генитив по идее, генитив для подстройки\n",
    "preps_cases['с'] = 'Case=Ins,Case=Gen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps_cases['на'] = 'Case=Loc,Case=Acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps_cases['за'] = 'Case=Ins,Case=Acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps_cases['через'] = 'Case=Acc,Case=Gen'\n",
    "# preps_cases['через'] = 'Case=Acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps_cases['со'] = 'Case=Ins,Case=Gen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps_cases['для'] = 'Case=Gen,Case=Acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps_cases['под'] = 'Case=Ins,Case=Acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Составные предлоги\n",
    "preps_cases['в из'] = ''\n",
    "preps_cases['на в'] = ''\n",
    "preps_cases['спустя после'] = ''\n",
    "\n",
    "preps_cases['без ведома'] = 'Case=Gen'\n",
    "preps_cases['близко от'] = 'Case=Gen'\n",
    "preps_cases['в виде'] = 'Case=Gen'\n",
    "preps_cases['в дополнение'] = ''\n",
    "preps_cases['в дополнение к'] = 'Case=Dat'\n",
    "preps_cases['в зависимости от'] = 'Case=Gen'\n",
    "preps_cases['в интересах'] = 'Case=Gen'\n",
    "preps_cases['в качестве'] = 'Case=Gen'\n",
    "preps_cases['в лице'] = 'Case=Gen'\n",
    "preps_cases['в отличие от'] = 'Case=Gen'\n",
    "preps_cases['в пользу'] = 'Case=Gen'\n",
    "preps_cases['в отношении'] = 'Case=Gen'\n",
    "preps_cases['в преддверии'] = 'Case=Gen'\n",
    "preps_cases['в продолжение'] = 'Case=Gen'\n",
    "preps_cases['в результате'] = 'Case=Gen'\n",
    "preps_cases['в роли'] = 'Case=Gen'\n",
    "preps_cases['в связи с'] = 'Case=Ins'\n",
    "preps_cases['в силу'] = 'Case=Gen'\n",
    "preps_cases['в случае'] = 'Case=Gen'\n",
    "preps_cases['в соответствии с'] = 'Case=Ins'\n",
    "preps_cases['в соответствии со'] = 'Case=Ins'\n",
    "preps_cases['в течение'] = 'Case=Gen'\n",
    "preps_cases['в целях'] = 'Case=Gen'\n",
    "preps_cases['в частности'] = ''\n",
    "preps_cases['в честь'] = 'Case=Gen'\n",
    "preps_cases['вдоль по'] = 'Case=Dat'\n",
    "preps_cases['в течение'] = 'Case=Gen'\n",
    "preps_cases['в ходе'] = 'Case=Gen'\n",
    "preps_cases['во'] = 'Case=Loc,Case=Acc'\n",
    "preps_cases['во имя'] = 'Case=Gen'\n",
    "preps_cases['во славу'] = 'Case=Gen'\n",
    "preps_cases['вплоть до'] = 'Case=Gen'\n",
    "preps_cases['впредь до'] = 'Case=Gen'\n",
    "preps_cases['вслед за'] = 'Case=Ins'\n",
    "preps_cases['впредь до'] = 'Case=Gen'\n",
    "preps_cases['за вычетом'] = 'Case=Gen'\n",
    "preps_cases['за исключением'] = 'Case=Gen'\n",
    "preps_cases['за счёт'] = 'Case=Gen'\n",
    "preps_cases['за счет'] = 'Case=Gen'\n",
    "preps_cases['за течение'] = 'Case=Gen'\n",
    "preps_cases['исходя из'] = 'Case=Gen'\n",
    "# preps_cases['ко'] = 'Case=Dat'\n",
    "preps_cases['ко'] = ''\n",
    "preps_cases['лицом к лицу с'] = 'Case=Ins'\n",
    "preps_cases['на благо'] = 'Case=Gen'\n",
    "preps_cases['на виду у'] = 'Case=Gen'\n",
    "preps_cases['на глазах у'] = 'Case=Gen'\n",
    "preps_cases['на предмет'] = 'Case=Gen'\n",
    "preps_cases['наряду с'] = 'Case=Ins'\n",
    "preps_cases['насчет'] = 'Case=Gen'\n",
    "preps_cases['начиная с'] = 'Case=Gen'\n",
    "preps_cases['не без'] = 'Case=Gen'\n",
    "preps_cases['не считая'] = 'Case=Gen'\n",
    "preps_cases['невзирая на'] = 'Case=Асс'\n",
    "preps_cases['недалеко от'] = 'Case=Gen'\n",
    "preps_cases['независимо от'] = 'Case=Gen'\n",
    "preps_cases['несмотря на'] = 'Case=Асс'\n",
    "preps_cases['об'] = 'Case=Loc'\n",
    "preps_cases['обо'] = 'Case=Loc,Case=Acc'\n",
    "preps_cases['от имени'] = 'Case=Gen'\n",
    "preps_cases['от лица'] = 'Case=Gen'\n",
    "preps_cases['по линии'] = 'Case=Gen'\n",
    "preps_cases['по мере'] = 'Case=Gen'\n",
    "preps_cases['по направлению к'] = 'Case=Dat'\n",
    "preps_cases['по отношению к'] = 'Case=Dat'\n",
    "preps_cases['по поводу'] = 'Case=Gen'\n",
    "preps_cases['по причине'] = 'Case=Gen'\n",
    "preps_cases['по случаю'] = 'Case=Gen'\n",
    "preps_cases['по сравнению с'] = 'Case=Ins'\n",
    "preps_cases['по сравнению со'] = 'Case=Ins'\n",
    "preps_cases['поблизости от'] = 'Case=Gen'\n",
    "preps_cases['под видом'] = 'Case=Gen'\n",
    "preps_cases['под эгидой'] = 'Case=Gen'\n",
    "preps_cases['при помощи'] = 'Case=Gen'\n",
    "preps_cases['применительно к'] = 'Case=Dat'\n",
    "preps_cases['рядом с'] = 'Case=Ins'\n",
    "preps_cases['с ведома'] = 'Case=Gen'\n",
    "preps_cases['с помощью'] = 'Case=Gen'\n",
    "preps_cases['с прицелом на'] = 'Case=Acc'\n",
    "preps_cases['с точки зрения'] = 'Case=Gen'\n",
    "preps_cases['с целью'] = 'Case=Gen'\n",
    "preps_cases['следом за'] = 'Case=Ins'\n",
    "preps_cases['смотря по'] = 'Case=Dat'\n",
    "preps_cases['согласно с'] = 'Case=Ins'\n",
    "preps_cases['со стороны'] = 'Case=Gen'\n",
    "preps_cases['судя по'] = 'Case=Dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = sorted(list(preps_cases.keys()), key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('russian_preps.txt', 'w', encoding='utf8') as f:\n",
    "    for x in ks:\n",
    "        f.write(x + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skills_2/RULEC-GEC_test_ya_speller_punct_comma-masked-no-lowercase_prep-masked_prep-noun-masked_parsed.txt', 'r', encoding='utf8') as f:\n",
    "    test_ya_parsed = f.read()\n",
    "test_parsed = test_ya_parsed.split('\\n\\n')\n",
    "del test_parsed[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skills_2/excluded_test_ya_speller_punct_prep-masked_prep-noun-masked_parsed.txt', 'r', encoding='utf8') as f:\n",
    "    test_ya_parsed = f.read()\n",
    "test_parsed = test_ya_parsed.split('\\n\\n')\n",
    "del test_parsed[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrepP agreement errors including составные предлоги и без числительных\n",
    "joint_sents = []\n",
    "err = 0\n",
    "for sent in test_parsed:\n",
    "    spl = sent.split('\\n')\n",
    "    synt_info = [x.split('\\t') for x in spl]\n",
    "    pos = [synt_info[i][3] for i in range(len(synt_info))]\n",
    "    heads = [synt_info[i][6] for i in range(len(synt_info))]\n",
    "    heads_pos = defaultdict(set)\n",
    "    for c, i in zip(heads, pos):\n",
    "        heads_pos[c].add(i)\n",
    "    i = 0\n",
    "    while i < len(synt_info):\n",
    "        tok = synt_info[i]\n",
    "        if (tok[3] == 'ADP') and (tok[7] == 'case') and (synt_info[int(tok[6])-1][3] == 'NOUN') and (\n",
    "        synt_info[int(tok[6])-1][7] == 'obl'):\n",
    "            first_letter = synt_info[int(tok[6])-1][1][0]\n",
    "            start = i\n",
    "            i += 1\n",
    "            while i < len(synt_info) and synt_info[i][7] == 'fixed':\n",
    "                i += 1\n",
    "            prep = ' '.join([x[1] for x in synt_info[start:i]]).lower()\n",
    "            if not preps_cases[prep]:\n",
    "                i += 1\n",
    "                continue\n",
    "            prep_features = preps_cases[prep].split(',')\n",
    "            noun_features = synt_info[int(tok[6])-1][5].split('|')\n",
    "            diff = [x for x in noun_features if x in prep_features]\n",
    "            try:\n",
    "                if len(diff) != 1:\n",
    "                    if len(prep_features) == 2:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    if 'NUM' in heads_pos[tok[6]]:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    err += 1\n",
    "                    fs = [features_dict[x.split('=')[1]] for x in noun_features if not x.startswith('Case')]\n",
    "                    fs.append(features_dict[prep_features[0].split('=')[1]])\n",
    "                    print(prep)\n",
    "                    print(fs)\n",
    "                    lexem = morph.parse(synt_info[int(tok[6])-1][2])\n",
    "                    lex = [x for x in lexem if 'NOUN' in x.tag and 'nomn' in x.tag][0]\n",
    "                    test = first_letter + lex.inflect(set(fs)).word[1:]\n",
    "                    synt_info[int(tok[6])-1][1] = test \n",
    "#                     print(sent)\n",
    "                    print(test) \n",
    "            except AttributeError:\n",
    "                print('lazzzzzzhhhhaaa')\n",
    "                print(prep_features)\n",
    "                print(noun_features)\n",
    "                print(sent)\n",
    "        i += 1\n",
    "    joint_sents.append(' '.join(x[1] for x in synt_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skills_2/RULEC-GEC_test_ya_speller_punct_comma-masked-no-lowercase_prep-masked_prep-noun-masked_prep-noun-agr.txt', 'w', encoding='utf8') as f:\n",
    "    for sent in joint_sents:\n",
    "        f.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RULEC-GEC_test_original_ya_corrected_prep_agreement.txt', 'r', encoding='utf8') as f:\n",
    "    test_prep = f.readlines()\n",
    "test_prep = [x.strip() for x in test_prep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RULEC-GEC_test_original_ya_corrected_prep_agreement_parsed.txt', 'w', encoding='utf8') as f:\n",
    "    for sent in test_prep:\n",
    "        f.write(model([sent])[0] + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdjNoun agreement errors (skills_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skills_2/RULEC-GEC_test_ya_speller_punct_comma-masked-no-lowercase_prep-masked_prep-noun-masked_prep-noun-agr_frame-v+noun_adj-masked_parsed.txt', 'r', encoding='utf8') as f:\n",
    "    test_ya_parsed = f.read()\n",
    "test_parsed = test_ya_parsed.split('\\n\\n')\n",
    "del test_parsed[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4993"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdjNoun agreement errors\n",
    "err = 0\n",
    "adj_joint_sents = []\n",
    "for sent in test_parsed:\n",
    "    spl = sent.split('\\n')\n",
    "    synt_info = [x.split('\\t') for x in spl]\n",
    "    deps = [synt_info[i][0] for i in range(len(synt_info))]\n",
    "    heads = [synt_info[i][6] for i in range(len(synt_info))]\n",
    "    heads_deps = defaultdict(set)\n",
    "    for h, d in zip(heads, deps):\n",
    "        heads_deps[h].add(d)\n",
    "    hd = dict(heads_deps)\n",
    "#     print(hd)\n",
    "    for x in synt_info:\n",
    "        first_letter = x[1][0]\n",
    "        if any(char.isdigit() for char in x[1]):\n",
    "            continue\n",
    "        if (x[3] == 'ADJ') and ((x[7] == 'amod') or (x[7] == 'root')) and (\n",
    "            len(x[5].split('|')) > 3) and ((synt_info[int(x[6])-1][3] == 'NOUN') or (x[6] == '0')) and (\n",
    "        (int(x[0]) - int(synt_info[int(x[6])-1][0])) != 1) and (synt_info[int(x[0])-2][3] != 'NUM'): \n",
    "#         synt_info[int(synt_info[int(x[6])-1][6])-1][3] != 'NOUN'):\n",
    "#             print(x)\n",
    "            try:                    \n",
    "                try:\n",
    "                    adj_deps = hd[str(x[0])]\n",
    "                    if any((synt_info[int(ind)-1][3] == 'PRON') or (synt_info[int(ind)-1][3] == 'ADJ') or (\n",
    "                        synt_info[int(ind)-1][3] == 'AUX') for ind in adj_deps):\n",
    "                        continue\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                adj_features = x[5].split('|')\n",
    "                if x[6] != '0':\n",
    "#                     if synt_info[int(x[6])-1][7] == 'appos':\n",
    "#                         new_case = [x for x in synt_info[int(synt_info[int(x[6])-1][6])-1][5].split('|') if x.startswith('Case')][0]\n",
    "#                         tmp_noun_features = synt_info[int(x[6])-1][5].split('|')\n",
    "#                         noun_features = [new_case if x.startswith('Case') else x for x in tmp_noun_features]\n",
    "#                     else:\n",
    "                    noun_features = synt_info[int(x[6])-1][5].split('|')\n",
    "                else:\n",
    "                    try:\n",
    "                        nsubj_ind = [synt_info[int(ind)-1][0] for ind in adj_deps if synt_info[int(ind)-1][7] == 'nsubj'][0]\n",
    "                        noun_features = synt_info[int(nsubj_ind)-1][5].split('|')\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "#                 print(common)\n",
    "                common = list(set(noun_features).intersection(adj_features))\n",
    "                diff = list(set(noun_features) - set(adj_features))\n",
    "                adj_diff = list(set(adj_features) - set(noun_features))\n",
    "                degree = [x for x in adj_diff if x.startswith('Degree=Sup') or x.startswith('Variant=Short')]\n",
    "                full = common + diff + degree\n",
    "#                 print(full)\n",
    "#                 print(diff)\n",
    "                if 'Variant=Short' in full:\n",
    "                    full = [x for x in full if not x.startswith('Case')]\n",
    "                if len(diff) > 1:\n",
    "#                 if diff:\n",
    "                    fs = [features_dict[x.split('=')[1]] for x in full if not x.startswith('Animacy')]\n",
    "#                     print(fs)\n",
    "                    if 'plur' in fs:\n",
    "                        if 'masc' in fs:\n",
    "                            fs.remove('masc')\n",
    "                        elif 'femn' in fs:\n",
    "                            fs.remove('femn')\n",
    "                        elif 'neut' in fs:\n",
    "                            fs.remove('neut')\n",
    "                    lexem = morph.parse(x[2])\n",
    "                    try:\n",
    "#                         if 'accs' in fs and 'masc' in fs and 'sing' in fs:\n",
    "#                             lex = [x for x in lexem if 'ADJF' in x.tag and 'accs' in x.tag][0]\n",
    "#                         else:\n",
    "                        lex = [x for x in lexem if 'ADJF' in x.tag][0]\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                    err += 1\n",
    "                    test = first_letter + lex.inflect(set(fs)).word[1:]\n",
    "#                     print(test)\n",
    "                    x[1] = test\n",
    "#                     print(sent)\n",
    "            except AttributeError:\n",
    "                print('LAZZZHHHAAAA')\n",
    "                print(degree)\n",
    "                print(full)\n",
    "                print(fs)\n",
    "                print(lex)\n",
    "                print(sent)\n",
    "    adj_joint_sents.append(' '.join(x[1] for x in synt_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skills_2/RULEC-GEC_test_ya_speller_punct_comma-masked-no-lowercase_prep-masked_prep-noun-masked_prep-noun-agr_frame-v+noun_adj-masked_adj-noun-agr.txt',\n",
    "          'w', encoding='utf8') as f:\n",
    "    for sent in adj_joint_sents:\n",
    "        f.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RULEC-GEC_test_original_ya_corrected_with_adj_agreement.txt', 'w', encoding='utf8') as f:\n",
    "    for sent in adj_joint_sents:\n",
    "        f.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RULEC-GEC_test_original_ya_corrected_prep_adj_agreement.txt', 'w', encoding='utf8') as f:\n",
    "    for sent in adj_joint_sents:\n",
    "        f.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skills_2/RULEC-GEC_test_ya_speller_punct_comma-masked-no-lowercase_prep-masked_prep-noun-masked_prep-noun-agr_frame-v+noun_adj-masked.txt', 'r', encoding='utf8') as f:\n",
    "    ya_prep_mask = f.readlines()\n",
    "ya_prep_mask = [x.strip() for x in ya_prep_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skills_2/RULEC-GEC_test_ya_speller_punct_comma-masked-no-lowercase_prep-masked_prep-noun-masked_prep-noun-agr_frame-v+noun_adj-masked_parsed.txt', 'w', encoding='utf8') as f:\n",
    "    for sent in ya_prep_mask:\n",
    "        f.write(model([sent])[0] + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skills_2/excluded_test_ya_speller_punct_prep-masked_prep-noun-masked_frame-v+noun.txt', 'r', encoding='utf8') as f:\n",
    "    excluded_test = f.readlines()\n",
    "excluded_test = [x.strip() for x in excluded_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_test = ['Хотя природные явления ( извержения , цветения водорослей , землетрясения , и т .',\n",
    "'Экологи показывали страшные фотографии и дали неопределенные угрозы , которые могут происходить на АЭС до или после строительства .',\n",
    "'плакат А . Николаева , за который арестовали его в 2012',\n",
    "'Работал с 1966 до 2008',\n",
    "'Общая площадь : 68 кв .',\n",
    "'В связи с тем , что эра Сталина произошло вместе со становлении Магадана , Магадан также стал частью механизмы террора и тирании сталинской власти .',\n",
    "'Какие плюсы и минусы существуют ? ( Независимость , гордость своей страной , возвращение национальной культуры / нереальность обучения на этих языках , падание от большого русско говоряшего мира , современность русского языка , и . т . д . )']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skills_2/excluded_test_ya_speller_punct_prep-masked_prep-noun-masked_frame-v+noun_parsed.txt', 'w', encoding='utf8') as f:\n",
    "    for sent in excluded_test:\n",
    "        f.write(model([sent])[0] + '\\n\\n')\n",
    "#         f.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cephfs/home/kravtsova/hse/thesis/RozovskayaRothTACL2018-dataset\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
